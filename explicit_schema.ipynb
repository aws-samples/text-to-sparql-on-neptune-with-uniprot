{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit Query: Query Uniprot using Langchain SPARQL chain\n",
    "Shows uniprot query using explicit schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Notebook Pre-Req\n",
    "\n",
    "You can upload this notebook into a Jupyter environment configured to use Neptune Workbench. I tested this on an Amazon Sagemaker notebook running Python 3.10.x. \n",
    "\n",
    "### Python Pre-Req\n",
    "\n",
    "I tested this on an Amazon Sagemaker notebook running Python 3.10.x. You need Python 3.9 or higher.\n",
    "\n",
    "### Neptune Pre-Req\n",
    "\n",
    "Your Neptune cluster must run engine version 1.2.x or higher\n",
    "\n",
    "### Anthropic\n",
    "\n",
    "Obtain API key from Anthropic and store in .env file\n",
    "https://python.langchain.com/v0.1/docs/integrations/platforms/anthropic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SELECTOR=\"anthropic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Langchain\n",
    "Need python 3.9 or greater plus langchain 0.0.341 ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# 3.9 or higher?\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Generation Prompt\n",
    "\n",
    "We want four prompts. Each prompt will include the schema. Additionally:\n",
    "- One prompt has no examples\n",
    "- One prompt has few shots\n",
    "- One prompt has tips\n",
    "- One prompt has few shots plus tips\n",
    "\n",
    "### Elements of the prompt\n",
    "Let's first build the optional parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "resources = Path.cwd() / \"resources\"\n",
    "tips = yaml.safe_load((resources / \"tips.yaml\").read_text())\n",
    "pfx = (Path.cwd() / \"resources\" / \"prefixes.txt\").read_text()\n",
    "ground_truth = yaml.safe_load((resources / \"ground-truth.yaml\").read_text())\n",
    "\n",
    "TIPS=\"\"\"\n",
    "Here are additional tips that you might find helpful:\n",
    "<tips>\n",
    "\"\"\"\n",
    "for t in tips:\n",
    "    TIPS += f\"<tip>{t.replace('{', '{{').replace('}', '}}')}</tip>\\n\"\n",
    "TIPS+=\"\"\"\n",
    "</tips>\n",
    "\"\"\"\n",
    "\n",
    "PREFIXES=f\"\"\"\n",
    "Include all of the following prefixes in the SPARQL query you generate:\n",
    "\n",
    "{pfx}\n",
    "\"\"\"\n",
    "\n",
    "def add_few_shot(idx):\n",
    "    hold_out = ground_truth[idx] \n",
    "    training_examples = ground_truth[:idx] + ground_truth[idx+1:]\n",
    "    assert hold_out[\"SPARQL\"] not in {x[\"SPARQL\"] for x in training_examples}\n",
    "    GT=\"\"\"\n",
    "Some examples:\n",
    "\n",
    "    \"\"\"\n",
    "    for t in training_examples:\n",
    "        GT += f\"\"\"\n",
    "<question>\n",
    "{t['question']}\n",
    "</question>\n",
    "\n",
    "<sparql>\n",
    "{t['SPARQL']}\n",
    "</sparql>\n",
    "\"\"\"\n",
    "    \n",
    "    return hold_out, GT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Gen Prompt\n",
    "Langchain SPARQL chain has a default prompt, but let's create one custom. Allow it to include tips and/or fewshots. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "def make_gen_prompt(arg_tips, arg_fewshot):\n",
    "    UNI_SPARQL_GENERATION_SELECT_TEMPLATE_INTRO = f\"\"\"Task: Generate a SPARQL SELECT statement for querying a graph database.\n",
    "    Convert an English language description of a question into a SPARQL query against the Uniprot knowledgebase that answers the question.\n",
    "    For instance, to find all taxa from the UniProt taxonomy, the following query in backticks would be suitable:\n",
    "\n",
    "    ```\n",
    "    SELECT ?taxon\n",
    "    WHERE\n",
    "    {{{{\n",
    "      ?taxon a up:Taxon .\n",
    "    }}}}\n",
    "    LIMIT 20\n",
    "    ```\n",
    "    \n",
    "    Instructions:\n",
    "    Use only the node types and properties provided in the schema.\n",
    "    Do not use any node types and properties that are not explicitly provided.\n",
    "    Limit the SPARQL results to 20. Add a LIMIT clause to the SPARQL query.\n",
    "    Include all necessary prefixes. The following set of prefixes can be used:\n",
    "    \n",
    "    ```\n",
    "    {PREFIXES}\n",
    "    ```\n",
    "\n",
    "    {arg_tips}\n",
    "\n",
    "\n",
    "    {arg_fewshot}\n",
    "\n",
    "    You can use the following keywords:\n",
    "\n",
    "    <keywords>\n",
    "        <keyword><ARN>keywords:5</ARN><name>Acetoin biosynthesis</name></keyword>\n",
    "        <keyword><ARN>keywords:47</ARN><name>Antifreeze protein</name></keyword>\n",
    "    </keywords>\n",
    "    \"\"\"\n",
    "\n",
    "    UNI_SPARQL_GENERATION_SELECT_TEMPLATE=UNI_SPARQL_GENERATION_SELECT_TEMPLATE_INTRO + \"\"\"\n",
    "    Schema:\n",
    "    {schema}\n",
    "    Note: Be as concise as possible.\n",
    "    Do not include any explanations or apologies in your responses.\n",
    "    Do not respond to any questions that ask for anything else than for you to construct a SPARQL query.\n",
    "    Do not include any text except the SPARQL query generated.\n",
    "\n",
    "    The question is:\n",
    "    {prompt}\"\"\"\n",
    "\n",
    "\n",
    "    UNI_SPARQL_GENERATION_SELECT_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"schema\", \"prompt\"], template=UNI_SPARQL_GENERATION_SELECT_TEMPLATE\n",
    "    )\n",
    "    \n",
    "    return UNI_SPARQL_GENERATION_SELECT_PROMPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Langchain Graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities as u\n",
    "from langchain_community.graphs import NeptuneRdfGraph\n",
    "\n",
    "# Grab Neptune cluster host/port from notebook instance environment variables\n",
    "GRAPH_NOTEBOOK_HOST= u.get_neptune_env(\"GRAPH_NOTEBOOK_HOST\")\n",
    "GRAPH_NOTEBOOK_PORT= u.get_neptune_env(\"GRAPH_NOTEBOOK_PORT\")\n",
    "AWS_REGION= u.get_neptune_env(\"AWS_REGION\")\n",
    "USE_IAM=u.get_neptune_env(\"GRAPH_NOTEBOOK_AUTH_MODE\")!=\"DEFAULT\"\n",
    "\n",
    "graph = NeptuneRdfGraph(\n",
    "    host=GRAPH_NOTEBOOK_HOST,\n",
    "    port=int(GRAPH_NOTEBOOK_PORT),\n",
    "    use_iam_auth=True,\n",
    "    region_name=AWS_REGION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and invoke chain\n",
    "### Define LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "llm = ChatAnthropic(model='claude-3-opus-20240229')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.chat_models import BedrockChat\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "MODEL_ID='anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "llm = BedrockChat(\n",
    "    model_id = MODEL_ID,\n",
    "    client = bedrock_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The chainmakers\n",
    "We'll need separate chains for each of the four options. Any fewshot needs a chain PER query, because we exclude from fewshot the query we are attempting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.graph_qa.neptune_sparql import NeptuneSparqlQAChain\n",
    "\n",
    "default_chain = NeptuneSparqlQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    sparql_prompt=make_gen_prompt(\"\", \"\"),\n",
    "    graph=graph, \n",
    "    verbose=False, \n",
    "    top_K=10, \n",
    "    return_intermediate_steps=True, \n",
    "    return_direct=True\n",
    ")\n",
    "\n",
    "make_default_chain = lambda index: default_chain\n",
    "\n",
    "tips_chain = NeptuneSparqlQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    sparql_prompt=make_gen_prompt(TIPS, \"\"),\n",
    "    graph=graph, \n",
    "    verbose=False, \n",
    "    top_K=10, \n",
    "    return_intermediate_steps=True, \n",
    "    return_direct=True\n",
    ")\n",
    "\n",
    "make_tips_chain = lambda index: tips_chain\n",
    "\n",
    "\n",
    "make_few_chain = lambda index: NeptuneSparqlQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    sparql_prompt=make_gen_prompt(\"\", add_few_shot(index)),\n",
    "    graph=graph, \n",
    "    verbose=False, \n",
    "    top_K=10, \n",
    "    return_intermediate_steps=True, \n",
    "    return_direct=True\n",
    ")\n",
    "\n",
    "make_tips_few_chain = lambda index: NeptuneSparqlQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    sparql_prompt=make_gen_prompt(TIPS, add_few_shot(index)),\n",
    "    graph=graph, \n",
    "    verbose=False, \n",
    "    top_K=10, \n",
    "    return_intermediate_steps=True, \n",
    "    return_direct=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test on each chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "def run_test(folder_name, chain_maker):\n",
    "\n",
    "    folder=f\"./{folder_name}\" \n",
    "    os.mkdir(folder)\n",
    "    def write_results(index, res):\n",
    "        with open(f\"{folder}/{index}.json\", 'w') as resfile: \n",
    "            resfile.write(json.dumps(res))\n",
    "\n",
    "    with open(f'{folder_name}_report.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"qfile\",\"question\", \"numresults\", \"errormsg\", \"gensparql\"])\n",
    "        for index, q in enumerate(ground_truth):\n",
    "            print(str(index))\n",
    "            nlq=q['question']\n",
    "            expected_sparql=q['SPARQL']\n",
    "            num_results=0\n",
    "            error_msg=\"\"\n",
    "            gen_sparql=\"\"\n",
    "            genres=None\n",
    "\n",
    "            try:\n",
    "                genres=chain_maker(index).invoke(nlq) # result has 'query', 'result/detailedMEssage, result/code, result.message, result.intermediateSteps\n",
    "                gen_sparql=genres['intermediate_steps'][0]['query'].replace(\"\\n\", \" \")\n",
    "                if 'results' in genres['result']:\n",
    "                    num_results = len(genres['result']['results']['bindings'])\n",
    "                if 'message' in genres['result']:\n",
    "                    error_msg=genres['result']['message'].replace(\"\\n\", \" \")\n",
    "                write_results(index, genres)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on {index}\")\n",
    "                print(\"Exception: {}\".format(type(e).__name__))\n",
    "                print(\"Exception message: {}\".format(e))\n",
    "                error_msg=\"Exception message: {}\".format(e).replace(\"\\n\", \" \")\n",
    "\n",
    "            writer.writerow([str(index), nlq, num_results, error_msg, gen_sparql])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\"schema\", make_default_chain)\n",
    "\n",
    "run_test(\"schema_tips\", make_tips_chain)\n",
    "\n",
    "run_test(\"schema_few\", make_few_chain)\n",
    "\n",
    "run_test(\"schema_few_tips\", make_few_tips_chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
