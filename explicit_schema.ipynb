{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run ground truth examples and get expected results\n",
    "We have a set of ground truth Uniprot queries in ```resources/groud-truth.yaml```. In that file, we map a natural language question to the equivalent SPARQL query to run on the Uniprot dataset. Here is an excerpt of that file:\n",
    "\n",
    "```\n",
    "question: Show me all transmembrane regions\n",
    "  SPARQL: |\n",
    "    SELECT  ?protein ?begin ?end\n",
    "    WHERE\n",
    "    {\n",
    "      ?protein  rdf:type     up:Protein ;\n",
    "                up:annotation  ?annotation .\n",
    "      ?annotation  rdf:type  up:Transmembrane_Annotation .\n",
    "      ?annotation up:range ?range .\n",
    "      ?range faldo:begin ?begin .\n",
    "      ?range faldo:end ?end .\n",
    "    }\n",
    "```\n",
    "\n",
    "In this notebook we simply run all the SPARQL queries to gather their results. Effectively we are retrieving \"ground truth\" results. In another notebook, we will ask the LLM to generate SPARQL queries from natural language queries. To measure how effectively the LLM generated the queries, we will compare the results of those queries to the ground truth results here. \n",
    "\n",
    "We run on two RDF databases:\n",
    "\n",
    "1. The public Uniprot SPARQL endpoint: https://sparql.uniprot.org/sparql\n",
    "2. Our own Neptune database cluster, loaded with Uniprot data. If you want to setup this in your account, follow the steps in README.md and run the notebook uniprot_loader.ipynb to load the files.\n",
    "\n",
    "See README.md for instructions how to setup notebook, Neptune cluster, and Bedrock to run these examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to run the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import boto3\n",
    "import utilities as u\n",
    "from pathlib import Path\n",
    "\n",
    "# get ground truth and prefixes\n",
    "resources = Path.cwd() / \"resources\"\n",
    "pfx = (Path.cwd() / \"resources\" / \"prefixes.txt\").read_text()\n",
    "ground_truth = yaml.safe_load((resources / \"ground-truth.yaml\").read_text())\n",
    "session=boto3.Session()\n",
    "\n",
    "# Build a SPARQL statemen that consists of: 1/ A big set of prefixes, 2/ the ground-truth SPARQL query (that does not include prefixes), 3/ a limit\n",
    "def form_sparql(q):\n",
    "    return f\"\"\"\n",
    "    {pfx}\n",
    "\n",
    "    {q['SPARQL']}\n",
    "\n",
    "    LIMIT 20\n",
    "\"\"\".strip()\n",
    "    \n",
    "# Run a single ground truth test against the Neptune cluster. \n",
    "# index locates one example from ground-truth.yaml\n",
    "# results are written to the file expected/{index}.json. It overwrites previous result, if any\n",
    "def run_one_test(index):\n",
    "\n",
    "    q = ground_truth[index]\n",
    "    nlq=q['question']\n",
    "    sparql=form_sparql(q)\n",
    "\n",
    "    try:\n",
    "        res=u.execute_sparql(sparql, session) \n",
    "        u.write_sparql_res(\"expected\", str(index), nlq, q['SPARQL'], sparql, res, \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {index}\")\n",
    "        print(\"Exception: {}\".format(type(e).__name__))\n",
    "        print(\"Exception message: {}\".format(e))\n",
    "        error_msg=\"Exception message: {}\".format(e).replace(\"\\n\", \" \")\n",
    "        u.write_sparql_res(\"expected\", str(index), nlq, q['SPARQL'], sparql, {}, error_msg)\n",
    "        print(error_msg)\n",
    "\n",
    "# Run all ground truth results against the Neptune cluster. Results are written to expected/*.json. \n",
    "def run_tests():\n",
    "    for index, q in enumerate(ground_truth):\n",
    "        run_one_test(index)\n",
    "\n",
    "# Run a single ground truth test against the public Uniprot SPARQL endpoint. \n",
    "# index locates one example from ground-truth.yaml\n",
    "# results are written to the file up/{index}.json. It overwrites previous result, if any\n",
    "def run_one_up_test(index):\n",
    "\n",
    "    q = ground_truth[index]\n",
    "    nlq=q['question']\n",
    "    sparql=form_sparql(q)\n",
    "\n",
    "    try:\n",
    "        res=u.execute_sparql_uniprotref(sparql)\n",
    "        u.write_sparql_res(\"up\", str(index), nlq, q['SPARQL'], sparql, res, \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {index}\")\n",
    "        print(\"Exception: {}\".format(type(e).__name__))\n",
    "        print(\"Exception message: {}\".format(e))\n",
    "        error_msg=\"Exception message: {}\".format(e).replace(\"\\n\", \" \")\n",
    "        u.write_sparql_res(\"up\", str(index), nlq, q['SPARQL'], sparql, {}, error_msg)\n",
    "        print(error_msg)\n",
    "\n",
    "# Run all ground truth results against the public Uniprot SPARQL endpoint. Results are written to up/*.json. \n",
    "def run_up_tests():\n",
    "    for index, q in enumerate(ground_truth):\n",
    "        run_one_up_test(index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make directories for expected and up results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf expected up\n",
    "mkdir -p expected up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the expected and up tests\n",
    "This will take several minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_up_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reports of results for expected and up tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u.make_report(\"expected\")\n",
    "u.make_report(\"up\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run just one test\n",
    "A convenient way to re-run a failed test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_one_up_test(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_one_test(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
