{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit Query: Query Uniprot using Langchain SPARQL chain\n",
    "Shows uniprot query using explicit schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Notebook Pre-Req\n",
    "\n",
    "You can upload this notebook into a Jupyter environment configured to use Neptune Workbench. I tested this on an Amazon Sagemaker notebook running Python 3.10.x. \n",
    "\n",
    "### Python Pre-Req\n",
    "\n",
    "I tested this on an Amazon Sagemaker notebook running Python 3.10.x. You need Python 3.9 or higher.\n",
    "\n",
    "### Neptune Pre-Req\n",
    "\n",
    "Your Neptune cluster must run engine version 1.2.x or higher\n",
    "\n",
    "### Anthropic\n",
    "\n",
    "Obtain API key from Anthropic and store in .env file\n",
    "https://python.langchain.com/v0.1/docs/integrations/platforms/anthropic/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Langchain\n",
    "Need python 3.9 or greater plus langchain 0.0.341 ish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# 3.9 or higher?\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --force-reinstall langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Generation Prompt\n",
    "\n",
    "We want four prompts. Each prompt will include the schema. Additionally:\n",
    "- One prompt has no examples\n",
    "- One prompt has few shots\n",
    "- One prompt has tips\n",
    "- One prompt has few shots plus tips\n",
    "\n",
    "### Elements of the prompt\n",
    "Let's first build the optional parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "resources = Path.cwd() / \"resources\"\n",
    "tips = yaml.safe_load((resources / \"tips.yaml\").read_text())\n",
    "pfx = (Path.cwd() / \"resources\" / \"prefixes.txt\").read_text()\n",
    "ground_truth = yaml.safe_load((resources / \"ground-truth.yaml\").read_text())\n",
    "\n",
    "TIPS=\"\"\"\n",
    "Here are additional tips that you might find helpful:\n",
    "<tips>\n",
    "\"\"\"\n",
    "for t in tips:\n",
    "    TIPS += f\"<tip>{t.replace('{', '{{').replace('}', '}}')}</tip>\\n\"\n",
    "TIPS+=\"\"\"\n",
    "</tips>\n",
    "\"\"\"\n",
    "\n",
    "PREFIXES=f\"\"\"\n",
    "Include all of the following prefixes in the SPARQL query you generate:\n",
    "\n",
    "{pfx}\n",
    "\"\"\"\n",
    "\n",
    "def add_few_shot(idx):\n",
    "    hold_out = ground_truth[idx] \n",
    "    training_examples = ground_truth[:idx] + ground_truth[idx+1:]\n",
    "    assert hold_out[\"SPARQL\"] not in {x[\"SPARQL\"] for x in training_examples}\n",
    "    GT=\"\"\"\n",
    "Some examples:\n",
    "\n",
    "    \"\"\"\n",
    "    for t in training_examples:\n",
    "        GT += f\"\"\"\n",
    "<question>\n",
    "{t['question']}\n",
    "</question>\n",
    "\n",
    "<sparql>\n",
    "{t['SPARQL']}\n",
    "</sparql>\n",
    "\"\"\"\n",
    "    \n",
    "    return GT.replace('{', '{{').replace('}', '}}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Gen Prompt\n",
    "Langchain SPARQL chain has a default prompt, but let's create one custom. Allow it to include tips and/or fewshots. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "def make_gen_prompt(arg_tips, arg_fewshot):\n",
    "    UNI_SPARQL_GENERATION_SELECT_TEMPLATE_INTRO = f\"\"\"Task: Generate a SPARQL SELECT statement for querying a graph database.\n",
    "    Convert an English language description of a question into a SPARQL query against the Uniprot knowledgebase that answers the question.\n",
    "    For instance, to find all taxa from the UniProt taxonomy, the following query in backticks would be suitable:\n",
    "\n",
    "    ```\n",
    "    SELECT ?taxon\n",
    "    WHERE\n",
    "    {{{{\n",
    "      ?taxon a up:Taxon .\n",
    "    }}}}\n",
    "    LIMIT 20\n",
    "    ```\n",
    "    \n",
    "    Instructions:\n",
    "    Use only the node types and properties provided in the schema.\n",
    "    Do not use any node types and properties that are not explicitly provided.\n",
    "    Limit the SPARQL results to 20. Add a LIMIT clause to the SPARQL query.\n",
    "    Include all necessary prefixes. The following set of prefixes can be used:\n",
    "    \n",
    "    ```\n",
    "    {PREFIXES}\n",
    "    ```\n",
    "\n",
    "    {arg_tips}\n",
    "\n",
    "\n",
    "    {arg_fewshot}\n",
    "\n",
    "    You can use the following keywords:\n",
    "\n",
    "    <keywords>\n",
    "        <keyword><ARN>keywords:5</ARN><name>Acetoin biosynthesis</name></keyword>\n",
    "        <keyword><ARN>keywords:47</ARN><name>Antifreeze protein</name></keyword>\n",
    "    </keywords>\n",
    "    \"\"\"\n",
    "    \n",
    "    UNI_SPARQL_GENERATION_SELECT_TEMPLATE=UNI_SPARQL_GENERATION_SELECT_TEMPLATE_INTRO + \"\"\"\n",
    "    Schema:\n",
    "    {schema}\n",
    "    Note: Be as concise as possible.\n",
    "    Do not include any explanations or apologies in your responses.\n",
    "    Do not respond to any questions that ask for anything else than for you to construct a SPARQL query.\n",
    "    Do not include any text except the SPARQL query generated.\n",
    "\n",
    "    The question is:\n",
    "    {prompt}\"\"\"\n",
    "\n",
    "    UNI_SPARQL_GENERATION_SELECT_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"schema\", \"prompt\"], template=UNI_SPARQL_GENERATION_SELECT_TEMPLATE\n",
    "    )\n",
    "    \n",
    "    return UNI_SPARQL_GENERATION_SELECT_PROMPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Langchain Graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities as u\n",
    "from langchain_community.graphs import NeptuneRdfGraph\n",
    "\n",
    "graph = NeptuneRdfGraph(\n",
    "    host=u.GRAPH_NOTEBOOK_HOST,\n",
    "    port=int(u.GRAPH_NOTEBOOK_PORT),\n",
    "    use_iam_auth=u.USE_IAM_AUTH,\n",
    "    region_name=u.AWS_REGION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and invoke chain\n",
    "### Define LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_aws import ChatBedrock\n",
    "#from langchain.chat_models import BedrockChat\n",
    "#from langchain.llms import Bedrock\n",
    "\n",
    "from botocore.config import Config\n",
    "\n",
    "config = Config(\n",
    "   retries = {\n",
    "      'max_attempts': 5,\n",
    "      'mode': 'standard'\n",
    "   }\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_ID='anthropic.claude-3-opus-20240229-v1:0'\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\", \n",
    "    region_name=u.AWS_REGION,\n",
    "    config=config\n",
    ")\n",
    "llm = ChatBedrock(\n",
    "    model_id = MODEL_ID,\n",
    "    client = bedrock_client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The chainmakers\n",
    "We'll need separate chains for each of the four options. Any fewshot needs a chain PER query, because we exclude from fewshot the query we are attempting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.graph_qa.neptune_sparql import NeptuneSparqlQAChain\n",
    "\n",
    "default_chain = NeptuneSparqlQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    sparql_prompt=make_gen_prompt(\"\", \"\"),\n",
    "    graph=graph, \n",
    "    verbose=False, \n",
    "    top_K=10, \n",
    "    return_intermediate_steps=True, \n",
    "    return_direct=True\n",
    ")\n",
    "\n",
    "make_default_chain = lambda index: default_chain\n",
    "\n",
    "tips_chain = NeptuneSparqlQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    sparql_prompt=make_gen_prompt(TIPS, \"\"),\n",
    "    graph=graph, \n",
    "    verbose=False, \n",
    "    top_K=10, \n",
    "    return_intermediate_steps=True, \n",
    "    return_direct=True\n",
    ")\n",
    "\n",
    "make_tips_chain = lambda index: tips_chain\n",
    "\n",
    "\n",
    "make_few_chain = lambda index: NeptuneSparqlQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    sparql_prompt=make_gen_prompt(\"\", add_few_shot(index)),\n",
    "    graph=graph, \n",
    "    verbose=False, \n",
    "    top_K=10, \n",
    "    return_intermediate_steps=True, \n",
    "    return_direct=True\n",
    ")\n",
    "\n",
    "make_tips_few_chain = lambda index: NeptuneSparqlQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    sparql_prompt=make_gen_prompt(TIPS, add_few_shot(index)),\n",
    "    graph=graph, \n",
    "    verbose=False, \n",
    "    top_K=10, \n",
    "    return_intermediate_steps=True, \n",
    "    return_direct=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test on each chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import utilities as u\n",
    "import time\n",
    "\n",
    "# In our testing our account had a throttle limit on Bedrock runtime model invocation.\n",
    "# Besides using boto3 for retries and backoff, we also introduced a sleep between calls.\n",
    "# Set this to -1 if you do NOT wish to sleep.\n",
    "SLEEP_INTERVAL_SECS=70\n",
    "\n",
    "def run_one_test(index, chain_maker, folder_name):\n",
    "    q=ground_truth[index]\n",
    "    nlq=q['question']\n",
    "    expected_sparql=q['SPARQL']\n",
    "    error_msg=\"\"\n",
    "    gen_sparql=\"\"\n",
    "    res=None\n",
    "\n",
    "    try:\n",
    "        res=chain_maker(index).invoke(nlq) # result has 'query', 'result/detailedMEssage, result/code, result.message, result.intermediateSteps\n",
    "        gen_sparql=res['intermediate_steps'][0]['query'].replace(\"\\n\", \" \")\n",
    "        if 'message' in res['result']:\n",
    "            error_msg=res['result']['message'].replace(\"\\n\", \" \")\n",
    "                        \n",
    "        if not(folder_name is None):\n",
    "            u.write_sparql_res(folder_name, str(index), nlq, expected_sparql, gen_sparql, res['result'], error_msg)\n",
    "        else:\n",
    "            print(nlq)\n",
    "            print(gen_sparql)\n",
    "            print(res)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {index}\")\n",
    "        print(\"Exception: {}\".format(type(e).__name__))\n",
    "        print(\"Exception message: {}\".format(e))\n",
    "        error_msg=\"Exception message: {}\".format(e).replace(\"\\n\", \" \")\n",
    "        if not(folder_name is None):\n",
    "            u.write_sparql_res(folder_name, str(index), nlq, q['SPARQL'], gen_sparql, [], error_msg)\n",
    "\n",
    "def run_tests(folder_name, chain_maker):\n",
    "\n",
    "    folder=f\"./{folder_name}\" \n",
    "    if not(os.path.exists(folder) and os.path.isdir(folder)):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    for index, q in enumerate(ground_truth):\n",
    "        if SLEEP_INTERVAL_SECS > 0:\n",
    "            time.sleep(SLEEP_INTERVAL_SECS)\n",
    "        print(f\"{folder_name} {str(index)}\")\n",
    "        run_one_test(index, chain_maker, folder_name)\n",
    "        \n",
    "def run_yourown_query(nlq):\n",
    "    res=make_tips_chain(-1).invoke(nlq) \n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests(\"schema_zero\", make_default_chain)\n",
    "\n",
    "run_tests(\"schema_tips\", make_tips_chain)\n",
    "\n",
    "run_tests(\"schema_few\", make_few_chain)\n",
    "\n",
    "run_tests(\"schema_few_tips\", make_few_tips_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.make_report(\"schema_zero\")\n",
    "u.make_report(\"schema_tips\")\n",
    "u.make_report(\"schema_few\")\n",
    "u.make_report(\"schema_few_tips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-off queries\n",
    "With schema and tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_yourown_query(\"Which proteins do frogs have\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
