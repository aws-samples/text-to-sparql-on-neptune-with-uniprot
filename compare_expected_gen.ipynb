{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd39a287-ec8a-467c-b7fe-d0d7641ba818",
   "metadata": {},
   "source": [
    "# Compare expected and actual SPARL results\n",
    "\n",
    "In this notebook, we check how well the LLM did at generating SPARQL queries on the Uniprot dataset. \n",
    "\n",
    "In ```get_expected_results.ipynb```, we ran each of the ground-truth queries against either the public Uniprot SPARQL endpoint or your own Neptune database to obtain expected SPARQL results for those queries. The results are in the ```up``` folder. \n",
    "\n",
    "In ```run_gen_tests.ipynb```, we asked the LLM to generate SPARQL queries for each ground-truth example. We ran the query against the Uniprot endpoint or your Neptune database. The results are in the ```gen_results``` folder. \n",
    "\n",
    "In this notebook, we compare the results. Are generated SPARQL queries similar to queries from ground truth? Are results similar?\n",
    "\n",
    "See included file ```comparison.html``` for a comparison of our test data. We also provide the full ```up``` and ```gen_results``` folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440b0bd-0c4f-45b5-95a0-cbd04b20f3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, re\n",
    "from itertools import count\n",
    "\n",
    "import pandas as pd\n",
    "import jinja2\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77e1d583-5729-48b6-90c4-dc89179b41d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jenv = jinja2.Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e0226fe-7ed4-4b01-b6a9-ddf07819f8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanup_line(sparql_line: str) -> str:\n",
    "    \"\"\" crude fixes to make it look better \"\"\"\n",
    "    if re.match(r\"^\\s+(?:SELECT|LIMIT|WHERE)\", sparql_line):\n",
    "        return sparql_line.strip()\n",
    "    else:\n",
    "        return sparql_line\n",
    "\n",
    "def remove_prefixes(sparql: str) -> str:\n",
    "    result = [line for line in sparql.split(\"\\n\") if not re.match(r\"^\\s*PREFIX\", line)]\n",
    "    # also remove empty lines\n",
    "    return \"\\n\".join(cleanup_line(line) for line in result if not re.match(r\"^\\s*$\", line))\n",
    "\n",
    "\n",
    "PREFIXES = {\n",
    "    \"taxon\": \"http://purl.uniprot.org/taxonomy/\",\n",
    "    \"uniprotkb\": \"http://purl.uniprot.org/uniprot/\"\n",
    "}\n",
    "\n",
    "def shorten_common_URIs(uri_str: str) -> str:\n",
    "    uri_str = uri_str.strip()\n",
    "    for prefix, uri in PREFIXES.items():\n",
    "        if uri_str.startswith(uri):\n",
    "            return f\"{prefix}:{uri_str[len(uri):]}\"\n",
    "    return uri_str\n",
    "\n",
    "\n",
    "def process_value(json_blob):\n",
    "    if json_blob[\"type\"] == \"uri\":\n",
    "        return shorten_common_URIs(json_blob[\"value\"])\n",
    "    elif json_blob[\"type\"] == \"literal\":\n",
    "        try:\n",
    "            return int(json_blob[\"value\"])\n",
    "        except:\n",
    "            try:\n",
    "                return float(json_blob[\"value\"])\n",
    "            except:\n",
    "                return str(json_blob[\"value\"])\n",
    "\n",
    " \n",
    "def results_to_data_frame(json_blob, problems) -> pd.DataFrame:\n",
    "    try:\n",
    "        res = json_blob[\"res\"]\n",
    "        if 'boolean' in res:\n",
    "            return  pd.DataFrame({\"boolean_result\": [ res[\"boolean\"] ] })\n",
    "        if  len(json_blob['error_type']) > 0:\n",
    "            problems.append(json_blob['error_type']) \n",
    "            return pd.DataFrame({\"error_msg\": [json_blob[\"error_type\"], json_blob[\"error_msg\"][0:40]]})\n",
    "        columns = res[\"head\"][\"vars\"]\n",
    "        data = {col: [] for col in columns}\n",
    "        for row in res[\"results\"][\"bindings\"]:\n",
    "            if len(row) > 0:\n",
    "                for col in columns:\n",
    "                    data[col].append(process_value(row.get(col, dict(type=\"literal\", value=\"\"))))\n",
    "        return pd.DataFrame(data=data)\n",
    "    except Exception as ex:\n",
    "        print(f\"Caught: {ex}\")\n",
    "        #print(json.dumps(json_blob, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb07c2-1183-4356-8b10-592fdfff8e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_value({\n",
    "            \"type\": \"literal\",\n",
    "            \"value\": \"http://rdf.wwpdb.org/pdb/6DZI\"\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b539f2e-5c6a-4f42-b4bd-c5e0535a9ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "resources = Path.cwd() / \"resources\"\n",
    "ground_truth = yaml.safe_load((resources / \"ground-truth.yaml\").read_text())\n",
    "\n",
    "expected={}\n",
    "results={}\n",
    "for p in (Path.cwd() / \"up\").glob(\"*.json\"):\n",
    "    if not(\"_\" in p.stem):\n",
    "        expected[int(p.stem)]=json.loads(p.read_text())\n",
    "for p in (Path.cwd() / \"gen_results\").glob(\"*.json\"):\n",
    "    if not(\"_\" in p.stem):\n",
    "        results[int(p.stem)]=json.loads(p.read_text())\n",
    "    \n",
    "    \n",
    "diff_results=[]\n",
    "\n",
    "for index, g in enumerate(ground_truth):\n",
    "    up_problems = []\n",
    "    res_problems = []\n",
    "    assert(index in expected)\n",
    "    assert(index in results)\n",
    "    assert('error_type' in expected[index])\n",
    "    assert('error_type' in results[index])\n",
    "    \n",
    "    diff_results.append({\n",
    "        'question': g['question'], \n",
    "        'expected_sparql': remove_prefixes(g['SPARQL']),\n",
    "        'expected_results': results_to_data_frame(expected[index], up_problems),\n",
    "        'actual_sparql': remove_prefixes(results[index]['actual_sparql']),\n",
    "        'actual_results': results_to_data_frame(results[index], res_problems)\n",
    "    })\n",
    "    diff_results[-1]['up_problems']=up_problems\n",
    "    diff_results[-1]['res_problems']=res_problems\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce91e18c-5dec-477a-ad80-30d1950f56a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import html\n",
    "def blob_to_table_html(iter) -> str:\n",
    "    \n",
    "    dr=diff_results[iter]\n",
    "    \n",
    "    template = jenv.from_string(\"\"\"\n",
    "    <table>\n",
    "      <thead>\n",
    "          <tr>\n",
    "              <th colspan=\"2\" style=\"text-align: center;\">{{question}}</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "              <th style=\"text-align: center;\">Expected</th>\n",
    "              <th style=\"text-align: center;\">Generated</th>\n",
    "          </tr>\n",
    "      <thead>\n",
    "      <tbody>\n",
    "        <tr>\n",
    "            <td><pre style=\"text-align: left;\">\n",
    "{{expected_sparql}}\n",
    "</pre></td>\n",
    "            <td><pre style=\"text-align: left;\">\n",
    "{{actual_sparql}}\n",
    "</pre></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>{{df_html_left}}</td>\n",
    "            <td>{{df_html_right}}</td>\n",
    "        </tr>\n",
    "      </tbody>\n",
    "    </table>\n",
    "    \"\"\")\n",
    "    return template.render(question=f\"{iter} {dr['question']}\",\n",
    "                           expected_sparql=html.escape(dr[\"expected_sparql\"]),\n",
    "                           actual_sparql=html.escape(dr[\"actual_sparql\"]),\n",
    "                           df_html_left=dr['expected_results'][0:30].to_html(),\n",
    "                           df_html_right=dr['actual_results'][0:30].to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d0e421-746b-4b5f-bd95-fca074862f35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "filter_set=[]\n",
    "html_str = \"\"\n",
    "\n",
    "for index, q in enumerate(ground_truth):\n",
    "    if len(filter_set) == 0 or (len(filter_set) > 0 and index in filter_set):\n",
    "        print(index)\n",
    "        html_str += \"\\n\" + blob_to_table_html(index)\n",
    "\n",
    "with open(\"comparison.html\", \"w\") as file1:\n",
    "    # Writing data to a file\n",
    "    file1.write(html_str)\n",
    "    \n",
    "display(HTML(html_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17630656-4a03-4441-9780-19f784d3aebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
